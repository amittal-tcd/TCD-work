{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Analytics Group 6.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMQJSzIR0Doi1SD5DXQ+L/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amittal-tcd/TCD-work/blob/master/Text%20Analytics/Step2%20-%20Model%20Improvement%20and%20Feature%20Importances/Text_Analytics_Group_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H14iij2s98tW",
        "colab_type": "text"
      },
      "source": [
        "## Install Libraries\n",
        "Need to run only once in a session\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGkJv2f_7vnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install catboost\n",
        "!pip install shap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5rPt87e-LPa",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries\n",
        "And download corpus which may only be done once in a session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS67FiKe7X3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import nltk\n",
        "from catboost import CatBoostRegressor, Pool, CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score as acc\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "dest = ''\n",
        "import shap\n",
        "from difflib import get_close_matches\n",
        "from sklearn.decomposition import PCA\n",
        "import io\n",
        "import requests\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(100)\n",
        "\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps9--aN7-VjY",
        "colab_type": "text"
      },
      "source": [
        "## Download Data from GIT and Keep Required Fields Only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKGHvsZe8LQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://github.com/amittal-tcd/TCD-work/raw/master/Text%20Analytics/Step2%20-%20Model%20Improvement%20and%20Feature%20Importances/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df2 = df[['reviews.rating','reviews.text','reviews.title','reviews.date']]\n",
        "\n",
        "s = np.random.uniform(high = df2.shape[0]-1, low = 0, size = round(0.2*df2.shape[0]))\n",
        "s = np.unique(s.round())\n",
        "\n",
        "dfv2 = df2.iloc[s,:] ## traing data\n",
        "dft2 = df2.drop(index = s) ## validation data\n",
        "\n",
        "print(df2.shape,dft2.shape,dfv2.shape)\n",
        "dft2.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwXTWT8f-mPe",
        "colab_type": "text"
      },
      "source": [
        "## Cleaning text\n",
        "- Removing punctuations and stop words\n",
        "- Experimentally removing words containing numbers\n",
        "- Removing non-english words\n",
        "- Stemming words so that there aren't similar words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRqNqN_U8aaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "words = set(nltk.corpus.words.words())\n",
        "ps = nltk.stem.PorterStemmer()\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "dft2['reviews.text'] = dft2['reviews.text'].map(lambda x: ' '.join([w.lower() for w in tokenizer.tokenize(x) if (not (w in stop_words)) and (w in words) and (any(char.isdigit() for char in w) == False)]))\n",
        "dft2.head()\n",
        "\n",
        "# ps.stem(w)\n",
        "# if (not (w in stop_words)) and (w in words) and (any(char.isdigit() for char in w) == False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGX8NtF4-z5x",
        "colab_type": "text"
      },
      "source": [
        "## Extracting Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPdvNvuE-46W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pca = PCA(n_components = 50)\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer='word',ngram_range=(1, 1))\n",
        "# vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,1),strip_accents = 'ascii')\n",
        "X = vectorizer.fit_transform(dft2['reviews.text'])\n",
        "X2 = vectorizer.transform(dfv2['reviews.text'])\n",
        "\n",
        "dft3 = pd.DataFrame(X.toarray())\n",
        "dfv3 = pd.DataFrame(X2.toarray())\n",
        "\n",
        "# dft3 = pd.DataFrame(pca.fit_transform(X.toarray()))\n",
        "# dfv3 = pd.DataFrame(pca.fit_transform(X2.toarray()))\n",
        "\n",
        "dft3.columns = vectorizer.get_feature_names()\n",
        "dfv3.columns = vectorizer.get_feature_names()\n",
        "\n",
        "dft3.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXcma-tb-9FF",
        "colab_type": "text"
      },
      "source": [
        "## Creating flag which is 1 if the date of review is on a weekday and 0 if it is on a weekend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wPXtNM6--tB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dft3['TimeCycle'] = np.where(pd.to_datetime(dft2['reviews.date'].str[:10], format='%Y-%m-%d').dt.dayofweek < 5,1,0)\n",
        "dfv3['TimeCycle'] = np.where(pd.to_datetime(dfv2['reviews.date'].str[:10], format='%Y-%m-%d').dt.dayofweek < 5,1,0)\n",
        "# df3['TimeCycle'] =  pd.to_datetime(df2['reviews.date'].str[:10], format='%Y-%m-%d').dt.day\n",
        "# df3['TimeCycle'] = df2['reviews.rating']\n",
        "dft3.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLZ9CDRJ_EJw",
        "colab_type": "text"
      },
      "source": [
        "## Train test validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTsC4Ns6_Fa5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dft3.drop(columns = ['TimeCycle']), dft3['TimeCycle'], test_size=0.20, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2EBTuA__NaN",
        "colab_type": "text"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "Change parameters for fine-tuning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYYCnbnB_M1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pool = Pool(data=X_train, label=y_train)\n",
        "test_pool = Pool(data=X_test, label=y_test.values)\n",
        "\n",
        "model = CatBoostClassifier(\n",
        "    iterations=5000,\n",
        "    learning_rate=0.1,\n",
        "    random_strength=0.1,\n",
        "    depth=6,\n",
        "    metric_period = 250,\n",
        "    eval_metric='AUC',\n",
        "    task_type = \"GPU\",\n",
        "    devices = '0:1'\n",
        ")\n",
        "                                       \n",
        "model.fit(train_pool,plot=True,eval_set=test_pool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE94u5hoV5NX",
        "colab_type": "text"
      },
      "source": [
        "## Grid Search for Model Improvement (Experimental - Last step for final model)\n",
        "\n",
        "Best 'params': {'depth': 6, 'iterations': 5000, 'learning_rate': 0.1} "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT2fJ8DUVewm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CatBoostClassifier(metric_period = 100,\n",
        "                            eval_metric='AUC',\n",
        "                            task_type = \"GPU\",\n",
        "                            devices = '0:1')\n",
        "    \n",
        "grid = {'learning_rate': [0.05, 0.1, 0.01],\n",
        "        'depth': [4, 6, 10],\n",
        "        'iterations': [2000,5000,1000]}\n",
        "\n",
        "randomized_search_result = model.randomized_search(grid, \n",
        "                                       X = dft3.drop(columns = ['TimeCycle']),\n",
        "                                       y = dft3['TimeCycle'], \n",
        "                                       plot=True)\n",
        "\n",
        "print(randomized_search_result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKcyg766ATPL",
        "colab_type": "text"
      },
      "source": [
        "## Making Prediction and calculating accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hFT4XSiATBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = model.predict(dfv3.drop(columns = 'TimeCycle')).round(0)\n",
        "final_accuracy = acc(dfv3['TimeCycle'],result)*100\n",
        "final_accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHk9tNd4Az2N",
        "colab_type": "text"
      },
      "source": [
        "## AUC and Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUM07BzhA1hc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "y_pred_proba = model.predict_proba(dfv3.drop(columns = 'TimeCycle'))[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(dfv3['TimeCycle'],y_pred_proba)\n",
        "auc = metrics.roc_auc_score(dfv3['TimeCycle'], y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"ROC, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()\n",
        "fig.savefig(dest+'ROC.png', dpi=fig.dpi)\n",
        "\n",
        "cm = metrics.confusion_matrix(dfv3['TimeCycle'],result)\n",
        "# labels = ['No Default', 'Default']\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot = True, fmt='d', cmap=\"Blues\", vmin = 0.2);\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Class')\n",
        "plt.xlabel('Predicted Class')\n",
        "plt.show()\n",
        "fig.savefig(dest+'Confusion Matrix.png', dpi=fig.dpi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wp0m7e0BEdZ",
        "colab_type": "text"
      },
      "source": [
        "## Plotting shap plots to check positive or negetive quantities of effect of the individual features for the final prediction\n",
        "\n",
        "### 1. Creating shap_values from validation data predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpebJj1CBHkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shap_values = model.get_feature_importance(Pool(dfv3.drop(columns = ['TimeCycle']), label=dfv3['TimeCycle']),type=\"ShapValues\")\n",
        "expected_value = shap_values[0,-1] ## Prediction Values\n",
        "shap_values = shap_values[:,:-1] ## Force Values for each feature. Therefore, excluding the prediction column"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPbXcas4CBP5",
        "colab_type": "text"
      },
      "source": [
        "### 2. Creating Summary of SHAP Values for Broad understanding of Impact of top Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmA9xFXdBeGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shap.summary_plot(shap_values, dfv3.drop(columns = ['TimeCycle']), max_display = 100) ## Change max_display value to add more features to the summary plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLYy54W8DxOS",
        "colab_type": "text"
      },
      "source": [
        "### 3. Lets look at a single prediction to verify results from summary plot\n",
        "\n",
        "#### First we draw a force plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCERDEA6D2M9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "row_number = 16  ## Change to look a specific row of validation data. Can also give a range for clustered force plot.\n",
        "\n",
        "shap.initjs()\n",
        "shap.force_plot(expected_value,shap_values[row_number,:],dfv3.drop(columns = ['TimeCycle']).iloc[row_number,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxZzA-VGHRf8",
        "colab_type": "text"
      },
      "source": [
        "#### Now, decision plot of the same prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEvEHT9sHV67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shap.decision_plot(expected_value,shap_values[row_number,:],dfv3.drop(columns = ['TimeCycle']).iloc[row_number,:], feature_names = list(dfv3.columns[:-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3xWasaqEzZ8",
        "colab_type": "text"
      },
      "source": [
        "## Looking at Overall Model Feature Importances\n",
        "\n",
        "Another way to look at importances apart from SHAP values. PredictionValuesChange for non-ranking metrics and LossFunctionChange for ranking metrics (the value is determined automatically)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D1guH69FEDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df5 = pd.DataFrame(data = {'Features': dft3.drop(columns = 'TimeCycle').columns, 'Importances':model.get_feature_importance()}).sort_values(by = 'Importances', ascending = False) ## Type in get importance by default is \"FeatureImportance\". Look at documentation for details.\n",
        "df5.to_csv(dest+'Importances.csv') ## Saved in session runtime. Can be seen in \"Files\" section in the margin on the left\n",
        "df5.head(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOAoLYNYGV_T",
        "colab_type": "text"
      },
      "source": [
        "## Point-Biscerial Correlations\n",
        "Another way to look relation between continuous features and binary target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY0jX0YCGeuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import stats\n",
        "\n",
        "l = []\n",
        "for i in dft3.drop(columns = 'TimeCycle'):\n",
        "    l2 = list(stats.pointbiserialr(dft3[i], dft3[\"TimeCycle\"]))\n",
        "    l2.append(i)\n",
        "    l.append(l2)\n",
        "\n",
        "df_corr = pd.DataFrame(l)\n",
        "df_corr.columns = ['Correlation', 'p.Value', 'Column']\n",
        "df_corr = df_corr.set_index('Column')\n",
        "df_corr = df_corr.sort_values('p.Value')\n",
        "df_corr[df_corr['p.Value'] <= 0.05].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9TjXFhRGp81",
        "colab_type": "text"
      },
      "source": [
        "### Lets look at the histogram of correlations to understand strength and direction of correlations of all significantly correlated features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHWs3A40Gxv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_corr2 = df_corr[df_corr['p.Value'] <= 0.05]\n",
        "fig = plt.figure()\n",
        "plt.hist(df_corr2['Correlation'], bins=50, color = 'g')\n",
        "plt.xlabel('Correlation Value')\n",
        "plt.ylabel('Density')\n",
        "plt.show()\n",
        "fig.savefig(dest+'fig5.png', dpi=fig.dpi)\n",
        "print(\"Number of significantly correlated features = \",df_corr2.shape[0])\n",
        "\n",
        "df_corr2['Correlation2'] = abs(df_corr2['Correlation'])\n",
        "df_corr3 = df_corr2.sort_values('Correlation2', ascending = False).drop(columns = 'Correlation2')\n",
        "\n",
        "print(\"\\nTop 100 significantly correlated features by correlation coefficient values\\n\")\n",
        "df_corr3.head(100)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}